{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EivkkOId_rA",
    "outputId": "7487933b-e977-4b51-edac-56531786807e"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiX-D3j-y9hW",
    "outputId": "93cb181b-2a71-4298-ee47-cda6eb221228"
   },
   "outputs": [],
   "source": [
    "!pip install -qU livelossplot\n",
    "!pip install -qU torchmetrics\n",
    "\n",
    "# final set\n",
    "# https://drive.google.com/file/d/1tcPv-KT09eMgYcM3YQVoRmIjdEb_Wgtc/view?usp=sharing\n",
    "!gdown 1tcPv-KT09eMgYcM3YQVoRmIjdEb_Wgtc\n",
    "!unzip -qq './document_dataset_resized.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTuuBgySzAtp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "import PIL\n",
    "\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torchmetrics import MeanMetric\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hr9rm1QkzDq9"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "seed = 41\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F2msBL_EzExM"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = r\"./document_dataset_resized\"\n",
    "\n",
    "BESTMODEL_PATH = r\"model_mbv3_iou_mix_2C_aux.pth\"  # path to save model weights\n",
    "\n",
    "IMAGE_SIZE = 384\n",
    "NUM_WORKERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FPm1hOxg9NL"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Pk4oIXmjzP8X"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as torchvision_T \n",
    "\n",
    "def train_transforms(mean=(0.4611, 0.4359, 0.3905), \n",
    "                      std=(0.2193, 0.2150, 0.2109)\n",
    "):\n",
    "    transforms = torchvision_T.Compose([\n",
    "        torchvision_T.ToTensor(),\n",
    "        torchvision_T.RandomGrayscale(p=0.4),                                        \n",
    "        torchvision_T.Normalize(mean, std),\n",
    "    ])\n",
    "    \n",
    "    return transforms\n",
    "\n",
    "\n",
    "def common_transforms(mean=(0.4611, 0.4359, 0.3905), \n",
    "                       std=(0.2193, 0.2150, 0.2109)\n",
    "):\n",
    "    transforms = torchvision_T.Compose([\n",
    "        torchvision_T.ToTensor(),\n",
    "        torchvision_T.Normalize(mean, std),\n",
    "    ])\n",
    "    \n",
    "    return transforms\n",
    "\n",
    "\n",
    "class SegDataset(Dataset):\n",
    "    def __init__(self, *, \n",
    "                 img_paths, \n",
    "                 mask_paths, \n",
    "                 image_size=(384, 384),  \n",
    "                 data_type=\"train\"\n",
    "    ):\n",
    "        self.data_type  = data_type\n",
    "        self.img_paths  = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.image_size = image_size\n",
    "\n",
    "\n",
    "        if self.data_type == \"train\":\n",
    "            self.transforms = train_transforms()\n",
    "        else:\n",
    "            self.transforms = common_transforms()\n",
    "\n",
    "    def read_file(self, path):\n",
    "        file = cv2.imread(path)[:, :, ::-1]\n",
    "        file = cv2.resize( file, \n",
    "                           self.image_size,                 \n",
    "                           interpolation=cv2.INTER_NEAREST\n",
    "                        )\n",
    "        return file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image_path = self.img_paths[index]\n",
    "        image = self.read_file(image_path)\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        mask_path = self.mask_paths[index]\n",
    "        \n",
    "        gt_mask = self.read_file(mask_path).astype(np.int32)\n",
    "\n",
    "        _mask = np.zeros((*self.image_size, 2), dtype=np.float32)\n",
    "        \n",
    "\t   # BACKGROUND\n",
    "        _mask[:, :, 0] = np.where(gt_mask[:, :, 0] == 0,   1.0, 0.0) \n",
    "\t   # DOCUMENT\n",
    "        _mask[:, :, 1] = np.where(gt_mask[:, :, 0] == 255, 1.0, 0.0)  \n",
    "\n",
    "        mask = torch.from_numpy(_mask).permute(2, 0, 1)\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8Gt9Vbb_5HsX"
   },
   "outputs": [],
   "source": [
    "def get_dataset(data_directory, batch_size=16):\n",
    "\n",
    "    train_img_dir = os.path.join(data_directory, \"train\", \"images\")\n",
    "    train_msk_dir = os.path.join(data_directory, \"train\", \"masks\")\n",
    "\n",
    "    valid_img_dir = os.path.join(data_directory, \"valid\", \"images\")\n",
    "    valid_msk_dir = os.path.join(data_directory, \"valid\", \"masks\")\n",
    " \n",
    " \n",
    "    train_img_paths = [os.path.join(train_img_dir, i) for i in os.listdir(train_img_dir)]\n",
    "    train_msk_paths = [os.path.join(train_msk_dir, i) for i in os.listdir(train_msk_dir)]\n",
    "\n",
    "    valid_img_paths = [os.path.join(valid_img_dir, i) for i in os.listdir(valid_img_dir)]\n",
    "    valid_msk_paths = [os.path.join(valid_msk_dir, i) for i in os.listdir(valid_msk_dir)]\n",
    "\n",
    "    train_ds = SegDataset(img_paths=train_img_paths, mask_paths=train_msk_paths, data_type=\"train\")\n",
    "    valid_ds = SegDataset(img_paths=valid_img_paths, mask_paths=valid_msk_paths, data_type=\"valid\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=NUM_WORKERS, shuffle=True,  pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=batch_size, num_workers=NUM_WORKERS, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x-n-ufHH5LuL"
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_dataset(DATA_DIR, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6PQHDWb5VgR",
    "outputId": "d2b13190-df6a-4fe9-9a29-6db2e9b3777b"
   },
   "outputs": [],
   "source": [
    "for i, j in valid_loader:\n",
    "    print(i.shape, j.shape, j.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nCCXdu1c-0xk"
   },
   "outputs": [],
   "source": [
    "def denormalize(tensors, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    \"\"\"Normalization parameters for pre-trained PyTorch models\n",
    "     Denormalizes image tensors using mean and std \"\"\"\n",
    "\n",
    "    for c in range(3):\n",
    "        tensors[:,c, :, :].mul_(std[c]).add_(mean[c])\n",
    "\n",
    "    return torch.clamp(tensors, min=0., max=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "hrFOxoIo5fIA",
    "outputId": "41933551-959b-416e-b4c4-86df818917d8"
   },
   "outputs": [],
   "source": [
    "for image, mask in valid_loader:\n",
    "    image = denormalize(image)\n",
    "    image = image[0]\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    x = image.permute(1, 2, 0).numpy()\n",
    "    plt.imshow(x)\n",
    "    plt.title('Image')\n",
    "    \n",
    "    labels = mask[0]\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(labels[0].numpy(), cmap='gray')\n",
    "    plt.title(\"Background Mask\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(labels[1].numpy(), cmap='gray')\n",
    "    plt.title(\"Document Mask\")\n",
    "    \n",
    "    plt.show()        \n",
    "    plt.close()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJPYTdXOhByb"
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EuCEwbjh-I64"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101 \n",
    "\n",
    "def prepare_model(backbone_model=\"mbv3\", num_classes=2):\n",
    "\n",
    "    # Initialize model with pre-trained weights.\n",
    "    weights = 'DEFAULT'\n",
    "    if backbone_model == \"mbv3\":\n",
    "        model = deeplabv3_mobilenet_v3_large(weights=weights)\n",
    "    elif backbone_model == \"r50\":\n",
    "        model = deeplabv3_resnet50(weights=weights)\n",
    "    elif backbone_model == \"r101\":\n",
    "        model = deeplabv3_resnet101(weights=weights)\n",
    "    else:\n",
    "        raise ValueError(\"Wrong backbone model passed. Must be one of 'mbv3', 'r50' and 'r101' \")\n",
    "\n",
    "    # Update the number of output channels for the output layer.\n",
    "    # This will remove the pre-trained weights for the last layer.\n",
    "    model.classifier[4]     = nn.LazyConv2d(num_classes, 1)\n",
    "    model.aux_classifier[4] = nn.LazyConv2d(num_classes, 1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "62914111841941b688924e4d74ff3e59",
      "1d00f82c60da4ff096d569cb1fc7b7bb",
      "b0278f0fdcd54565b8a0a52989d32d7b",
      "fb65e38a4a2d4c4eabd34fb3ec96bd9f",
      "3f50ecebb3d4443e8bbdb602137e2693",
      "8ede4aa8b2a64aeaaca3b83feed34a9a",
      "22585072f4d6464784249b2e6d0dea8b",
      "e9f3bf23e753452cb53ee9eb208645bf",
      "c5105d9aed7c4c719850f869c5f5cbff",
      "90667b73386945a3852e52f6d14cd38c",
      "bd793aedb2284431a62ccdb93f6599ed"
     ]
    },
    "id": "yhZFBeEFKIP3",
    "outputId": "0ddc938e-61e6-4521-d843-a7c8e9292bf9"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "model = prepare_model(num_classes=2)\n",
    "\n",
    "model.train()\n",
    "out = model(torch.randn((2, 3, 384, 384)))\n",
    "out['out'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bijiekk1hF8u"
   },
   "source": [
    "# Loss and Metric Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5zkmI_rA-eY0"
   },
   "outputs": [],
   "source": [
    "def intermediate_metric_calculation(\n",
    "    predictions, targets, use_dice=False, smooth=1e-6, dims=(2, 3)\n",
    "):\n",
    "    # dimscorresponding to image height and width: [B, C, H, W].\n",
    "    \n",
    "    # Intersection: |G ∩ P|. Shape: (batch_size, num_classes)\n",
    "    intersection = (predictions * targets).sum(dim=dims) + smooth \n",
    "\n",
    "    # Summation: |G| + |P|. Shape: (batch_size, num_classes).\n",
    "    summation = (predictions.sum(dim=dims) + targets.sum(dim=dims)) + smooth \n",
    "        \n",
    "    if use_dice:\n",
    "        # Dice Shape: (batch_size, num_classes) \n",
    "        metric = (2.0 * intersection) / summation\n",
    "    else:\n",
    "        # Union. Shape: (batch_size, num_classes)\n",
    "        union = summation - intersection\n",
    "\n",
    "        # IoU Shape: (batch_size, num_classes)\n",
    "        metric = intersection /  union\n",
    "        \n",
    "    # Compute the mean over the remaining axes (batch and classes). \n",
    "    # Shape: Scalar\n",
    "    total = metric.mean()\n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FC8lXzjATOff"
   },
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6, use_dice=False):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.use_dice = use_dice\n",
    "\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # predictions --> (B, #C, H, W) unnormalized\n",
    "        # targets     --> (B, #C, H, W) one-hot encoded\n",
    "\n",
    "        # Normalize model predictions\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "\n",
    "        # Calculate pixel-wise loss for both channels. Shape: Scalar\n",
    "        pixel_loss = F.binary_cross_entropy(predictions, targets, reduction=\"mean\")\n",
    "        \n",
    "        mask_loss  = 1 - intermediate_metric_calculation(predictions, targets, use_dice=self.use_dice, smooth=self.smooth)\n",
    "        total_loss = mask_loss + pixel_loss\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qRztrgtGTQNC"
   },
   "outputs": [],
   "source": [
    "def convert_2_onehot(matrix, num_classes=3):\n",
    "    '''\n",
    "    Perform one-hot encoding across the channel dimension.\n",
    "    '''\n",
    "    matrix = matrix.permute(0, 2, 3, 1)\n",
    "    matrix = torch.argmax(matrix, dim=-1)\n",
    "    matrix = torch.nn.functional.one_hot(matrix, num_classes=num_classes)\n",
    "    matrix = matrix.permute(0, 3, 1, 2)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "class Metric(nn.Module):\n",
    "    def __init__(self, num_classes=3, smooth=1e-6, use_dice=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.smooth      = smooth\n",
    "        self.use_dice    = use_dice\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # predictions  --> (B, #C, H, W) unnormalized\n",
    "        # targets      --> (B, #C, H, W) one-hot encoded \n",
    "\n",
    "        # Converting unnormalized predictions into one-hot encoded across channels.\n",
    "        # Shape: (B, #C, H, W) \n",
    "        predictions = convert_2_onehot(predictions, num_classes=self.num_classes) # one hot encoded\n",
    "\n",
    "        metric = intermediate_metric_calculation(predictions, targets, use_dice=self.use_dice, smooth=self.smooth)\n",
    "        \n",
    "        # Compute the mean over the remaining axes (batch and classes). Shape: Scalar\n",
    "        return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj9i3mO8hKGw"
   },
   "source": [
    "# Training Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aw0_6ZSxzHeD"
   },
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "def get_default_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kK_XEA_lA0HP"
   },
   "outputs": [],
   "source": [
    "def step(model, epoch_num=None, loader=None, optimizer_fn=None, loss_fn=None, metric_fn=None, is_train=False, metric_name=\"iou\"):\n",
    "\n",
    "    loss_record   = MeanMetric()\n",
    "    metric_record = MeanMetric()\n",
    "    \n",
    "    loader_len = len(loader)\n",
    "\n",
    "    text = \"Train\" if is_train else \"Valid\"\n",
    "\n",
    "    for data in tqdm(iterable=loader, total=loader_len, dynamic_ncols=True, desc=f\"{text} :: Epoch: {epoch_num}\"):\n",
    "        \n",
    "        if is_train:\n",
    "            preds = model(data[0])[\"out\"]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                preds = model(data[0])[\"out\"].detach()\n",
    "\n",
    "        loss = loss_fn(preds, data[1])\n",
    "\n",
    "        if is_train:\n",
    "            optimizer_fn.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_fn.step()\n",
    "\n",
    "        metric = metric_fn(preds.detach(), data[1])\n",
    "\n",
    "        loss_value = loss.detach().item()\n",
    "        metric_value = metric.detach().item()\n",
    "        \n",
    "        loss_record.update(loss_value)\n",
    "        metric_record.update(metric_value)\n",
    "\n",
    "    current_loss   = loss_record.compute()\n",
    "    current_metric = metric_record.compute()\n",
    "\n",
    "    # print(f\"\\rEpoch {epoch:>03} :: TRAIN :: LOSS: {loss_record.compute()}, {metric_name.upper()}: {metric_record.compute()}\\t\\t\\t\\t\", end=\"\")\n",
    "\n",
    "    return current_loss, current_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOHDs3pwhVzB"
   },
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTLDmGE8v2wV",
    "outputId": "46461b29-f953-4a8c-cd57-f501632f9fb7"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2 # 50\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "device = get_default_device()\n",
    "\n",
    "backbone_model_name = \"mbv3\" # mbv3 | r50 | r101\n",
    "\n",
    "model = prepare_model(backbone_model=backbone_model_name, num_classes=NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "# Dummy pass through the model\n",
    "_ = model(torch.randn((2, 3, 384, 384), device=device))\n",
    "\n",
    "\n",
    "train_loader, valid_loader = get_dataset(data_directory=DATA_DIR, batch_size=BATCH_SIZE)\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "valid_loader = DeviceDataLoader(valid_loader, device)\n",
    "\n",
    "metric_name = \"iou\"\n",
    "use_dice = True if metric_name == \"dice\" else False \n",
    "\n",
    "metric_fn = Metric(num_classes=NUM_CLASSES, use_dice=use_dice).to(device)\n",
    "loss_fn   = Loss(use_dice=use_dice).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "NSkKFLBNAoQK",
    "outputId": "de24a53d-b9db-4dcc-f13b-ac7a7ed89f8d"
   },
   "outputs": [],
   "source": [
    "liveloss = PlotLosses()  \n",
    "\n",
    "best_metric = 0.0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "\n",
    "    logs = {}\n",
    "\n",
    "    model.train()\n",
    "    train_loss, train_metric = step(model, \n",
    "                                    epoch_num=epoch, \n",
    "                                    loader=train_loader, \n",
    "                                    optimizer_fn=optimizer, \n",
    "                                    loss_fn=loss_fn, \n",
    "                                    metric_fn=metric_fn, \n",
    "                                    is_train=True,\n",
    "                                    metric_name=metric_name,\n",
    "                                    )\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss, valid_metric = step(model, \n",
    "                                    epoch_num=epoch, \n",
    "                                    loader=valid_loader, \n",
    "                                    loss_fn=loss_fn, \n",
    "                                    metric_fn=metric_fn, \n",
    "                                    is_train=False,\n",
    "                                    metric_name=metric_name,\n",
    "                                    )\n",
    "\n",
    "    logs['loss']               = train_loss\n",
    "    logs[metric_name]          = train_metric\n",
    "    logs['val_loss']           = valid_loss\n",
    "    logs[f'val_{metric_name}'] = valid_metric\n",
    "\n",
    "    liveloss.update(logs)\n",
    "    liveloss.send()\n",
    "\n",
    "    if valid_metric >= best_metric:\n",
    "        print(\"\\nSaving model.....\")\n",
    "        torch.save(model.state_dict(), BESTMODEL_PATH)\n",
    "        best_metric = valid_metric"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Custom training deeplabv3-mbv3",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow280')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "75aa9c6daad11fbe7704e5dc5bd12ef893ea958e6c5f653ac1e3d84cdd3c4b71"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1d00f82c60da4ff096d569cb1fc7b7bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ede4aa8b2a64aeaaca3b83feed34a9a",
      "placeholder": "​",
      "style": "IPY_MODEL_22585072f4d6464784249b2e6d0dea8b",
      "value": "100%"
     }
    },
    "22585072f4d6464784249b2e6d0dea8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f50ecebb3d4443e8bbdb602137e2693": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62914111841941b688924e4d74ff3e59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d00f82c60da4ff096d569cb1fc7b7bb",
       "IPY_MODEL_b0278f0fdcd54565b8a0a52989d32d7b",
       "IPY_MODEL_fb65e38a4a2d4c4eabd34fb3ec96bd9f"
      ],
      "layout": "IPY_MODEL_3f50ecebb3d4443e8bbdb602137e2693"
     }
    },
    "8ede4aa8b2a64aeaaca3b83feed34a9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90667b73386945a3852e52f6d14cd38c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0278f0fdcd54565b8a0a52989d32d7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9f3bf23e753452cb53ee9eb208645bf",
      "max": 44356159,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5105d9aed7c4c719850f869c5f5cbff",
      "value": 44356159
     }
    },
    "bd793aedb2284431a62ccdb93f6599ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5105d9aed7c4c719850f869c5f5cbff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e9f3bf23e753452cb53ee9eb208645bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb65e38a4a2d4c4eabd34fb3ec96bd9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90667b73386945a3852e52f6d14cd38c",
      "placeholder": "​",
      "style": "IPY_MODEL_bd793aedb2284431a62ccdb93f6599ed",
      "value": " 42.3M/42.3M [00:00&lt;00:00, 124MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
